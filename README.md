**LLM Classification Finetuning**
Fine-tune LLMs to Predict Human Preference using Chatbot Arena conversations

This competition challenges you to predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). 
You'll be given a dataset of conversations from the Chatbot Arena, where different LLMs generate answers to user prompts. By developing a winning machine learning model, 
you'll help improve how chatbots interact with humans and ensure they better align with human preferences.

This challenge aligns with the concept of "reward models" or "preference models" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias).

This competition challenges you to predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs).

Here is the link to the competition: https://www.kaggle.com/competitions/llm-classification-finetuning
